{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Normal Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets trl peft bitsandbytes==0.45.0 \n",
    "!pip install --upgrade pip\n",
    "!pip install unsloth unsloth_zoo --no-cache-dir --upgrade\n",
    "!pip install vllm\n",
    "!pip install --upgrade pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-10 03:08:55 __init__.py:207] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.49.0. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.168 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit with actual GPU utilization = 49.49%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 22.17 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 1024. Num Sequences = 224.\n",
      "Unsloth: vLLM's KV Cache can use up to 8.55 GB. Also swap space = 6 GB.\n",
      "INFO 04-10 03:09:04 config.py:549] This model supports multiple tasks: {'embed', 'classify', 'score', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.2.mlp', 'model.layers.3.mlp', 'model.layers.30.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 04-10 03:09:05 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":224}, use_cached_outputs=False, \n",
      "INFO 04-10 03:09:05 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 04-10 03:09:05 model_runner.py:1110] Starting to load model unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit...\n",
      "INFO 04-10 03:09:06 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W410 03:09:05.168625583 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-10 03:09:06 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb0ed3c22a64aec862e054281e4658d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd8512916534e5e949367f0eb5991ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-10 03:09:08 model_runner.py:1115] Loading model weights took 2.2160 GB\n",
      "INFO 04-10 03:09:08 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 04-10 03:09:10 worker.py:267] Memory profiling takes 2.46 seconds\n",
      "INFO 04-10 03:09:10 worker.py:267] the current vLLM instance can use total_gpu_memory (22.17GiB) x gpu_memory_utilization (0.49) = 10.97GiB\n",
      "INFO 04-10 03:09:10 worker.py:267] model weights take 2.22GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 1.23GiB; the rest of the memory reserved for KV Cache is 7.48GiB.\n",
      "INFO 04-10 03:09:11 executor_base.py:111] # cuda blocks: 13623, # CPU blocks: 10922\n",
      "INFO 04-10 03:09:11 executor_base.py:116] Maximum concurrency for 1024 tokens per request: 212.86x\n",
      "INFO 04-10 03:09:16 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:26<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-10 03:09:43 model_runner.py:1562] Graph capturing finished in 26 secs, took 4.41 GiB\n",
      "INFO 04-10 03:09:43 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 35.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth 2025.3.19 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "import torch\n",
    "from transformers import AutoTokenizer, TextStreamer\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "def load_model_tokenizer_ft(base_model_id=\"Qwen/Qwen2.5-3B-Instruct\", adapter_path=\"grpo_lora\"):\n",
    "    # Load the base model\n",
    "    base_model, _ = FastLanguageModel.from_pretrained(\n",
    "        model_name=base_model_id,\n",
    "        max_seq_length=1024,         # Same as during training\n",
    "        load_in_4bit=True,           # Load in 4-bit for memory efficiency\n",
    "        fast_inference=True,         # Enable fast inference\n",
    "    )\n",
    "\n",
    "    # Apply LoRA weights (need to be the same as before)\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        base_model,\n",
    "        r=16,  # Same rank as during training\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "        lora_alpha=32,  # Same alpha as during training\n",
    "        use_gradient_checkpointing=\"unsloth\",  # Enable gradient checkpointing if needed\n",
    "        random_state=3407,  # Same random state as during training\n",
    "    )\n",
    "    # Load the saved LoRA weights\n",
    "    model.load_adapter(adapter_path,adapter_name=\"default\")\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "    return base_model, model, tokenizer\n",
    "\n",
    "base_model, model,tokenizer=  load_model_tokenizer_ft()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize input question\n",
    "def format_prompt(question):\n",
    "    system_prompt = (\n",
    "    \"You are an expert in solving high school math word problems. \"\n",
    "    \"You first think through the reasoning process step-by-step in your mind and then provide the answer.\"\n",
    "    )\n",
    "\n",
    "    user_prompt= (\n",
    "    \"Solve the following question:\\n{ques}.\\n\"\n",
    "    \"Show your work in <think> </think> tags. And return the final answer as an integer in <answer> </answer> tags.\"\n",
    "    )\n",
    "\n",
    "    prompt=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt.format(ques=question)},\n",
    "        {\"role\": \"assistant\", \"content\": \"Let me solve this step by step.\\n<think>\"}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(prompt,tokenize=False, continue_final_message=True)\n",
    "\n",
    "\n",
    "def inference(question):\n",
    "    prompt=format_prompt(question)\n",
    "    # tokenize the prompt\n",
    "    input_ids=tokenizer(prompt,return_tensors=\"pt\").input_ids\n",
    "    input_ids=input_ids.to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.generate(\n",
    "            input_ids,\n",
    "            max_length=1024,  # Adjust based on your needs\n",
    "            temperature=0.1,  # Control randomness (lower = more deterministic)\n",
    "            top_p=0.9,        # Take the top logits with probs summing up to 0.9\n",
    "            do_sample=True,\n",
    "            streamer = TextStreamer(tokenizer, skip_special_tokens=True)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are an expert in solving high school math word problems. You first think through the reasoning process step-by-step in your mind and then provide the answer.\n",
      "user\n",
      "Solve the following question:\n",
      "James trains for the Olympics. He trains twice a day for 4 hours each time for all but 2 days per week. How many hours does he train a year?.\n",
      "Show your work in <think> </think> tags. And return the final answer as an integer in <answer> </answer> tags.\n",
      "assistant\n",
      "Let me solve this step by step.\n",
      "<think> First, we need to determine how many days James trains in a week. He trains for 7 days minus the 2 days he doesn't train, which means he trains for 5 days a week. \n",
      "\n",
      "Next, we calculate the total training hours per training day. He trains for 4 hours each time, and he trains twice a day, so he trains for 4 * 2 = 8 hours per training day.\n",
      "\n",
      "Now, we multiply the number of training days per week by the number of hours he trains each day to find the total training hours per week. That is, 5 days * 8 hours/day = 40 hours/week.\n",
      "\n",
      "Finally, to find out how many hours he trains in a year, we multiply the weekly training hours by the number of weeks in a year. Assuming a year has 52 weeks, the total training hours in a year would be 40 hours/week * 52 weeks/year = 2080 hours/year.\n",
      "</think>\n",
      "<answer>2080</answer>\n"
     ]
    }
   ],
   "source": [
    "question=\"James trains for the Olympics. He trains twice a day for 4 hours each time for all but 2 days per week. How many hours does he train a year?\"\n",
    "inference(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inference with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install pypdf\n",
    "!pip install -U langchain-community\n",
    "!pip install chromadb\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n",
      "page_content='Word Problem \n",
      "Practice Workbook\n",
      "00i_TP_881033.indd Page 1  1/15/08  10:45:43 AM user00i_TP_881033.indd Page 1  1/15/08  10:45:43 AM user /Volumes/ju104/MHGL149/Quark%0/Word Problem%/Application file%0/FM/Course 1/Volumes/ju104/MHGL149/Quark%0/Word Problem%/Application file%0/FM/Course 1\n",
      "PDF Proof' metadata={'producer': 'Acrobat Distiller 7.0 for Macintosh', 'creator': 'QuarkXPressâ„¢ 4.1 w/d: LaserWriter 8 8.7.3', 'creationdate': '2007-12-24T15:16:33+05:30', 'author': 'qwe', 'moddate': '2008-01-15T15:37:24+05:30', 'title': '001_009_CRM01_881033.qxd', 'source': 'decimalwordprobsgood.pdf', 'total_pages': 104, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader =PyPDFLoader(\"decimalwordprobsgood.pdf\")\n",
    "pages=loader.load()\n",
    "print(len(pages))\n",
    "print(pages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409\n",
      "page_content='Word Problem \n",
      "Practice Workbook\n",
      "00i_TP_881033.indd Page 1  1/15/08  10:45:43 AM user00i_TP_881033.indd Page 1  1/15/08  10:45:43 AM user /Volumes/ju104/MHGL149/Quark%0/Word Problem%/Application file%0/FM/Course 1/Volumes/ju104/MHGL149/Quark%0/Word Problem%/Application file%0/FM/Course 1\n",
      "PDF Proof' metadata={'producer': 'Acrobat Distiller 7.0 for Macintosh', 'creator': 'QuarkXPressâ„¢ 4.1 w/d: LaserWriter 8 8.7.3', 'creationdate': '2007-12-24T15:16:33+05:30', 'author': 'qwe', 'moddate': '2008-01-15T15:37:24+05:30', 'title': '001_009_CRM01_881033.qxd', 'source': 'decimalwordprobsgood.pdf', 'total_pages': 104, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 50,\n",
    ")\n",
    "splits=splitter.split_documents(pages)\n",
    "print(len(splits))\n",
    "print(splits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = 'cs229_lectures/chroma/'\n",
    "!rm -rf ./docs/chroma  # remove old database files if any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1911/65260539.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_function = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1602\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Use SentenceTransformer as a LangChain-compatible embedding function\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\")\n",
    "\n",
    "# Create ChromaDB vector store\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,  # Your list of text documents\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=persist_directory  # Path to store embeddings\n",
    ")\n",
    "\n",
    "print(vectordb._collection.count())  # Check number of stored vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model,tokenizer,question):\n",
    "    system_prompt=\"You are a helpful assistant. You first think through the reasoning process step-by-step in your mind and then provide the answer.\"\n",
    "    \n",
    "    # search vectordb for meaningful context: use similarity_search_with_scores\n",
    "    results_with_scores=vectordb.similarity_search_with_score(question,k=3)\n",
    "    # filter documents with high scores\n",
    "    relevant_results = [doc for doc, score in results_with_scores if score >= 0.3]\n",
    "\n",
    "    if relevant_results:\n",
    "        context = \"\\n\\n\".join([d.page_content for d in relevant_results])\n",
    "    else:\n",
    "        context=\"\"\n",
    "    # user prompt\n",
    "    template = \"\"\"Use the following pieces of context to answer the question at the end. If the context is None, simply answer the question based on your knowledge. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "    Show your reasoning in <think> </think> tags. And return the final answer in <answer> </answer> tags. \n",
    "    {context}\n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    "    user_prompt=template.format(context=context,question=question)\n",
    "    \n",
    "    prompt=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": \"Let me solve this step by step.\\n<think>\"}\n",
    "    ]\n",
    "    # set input text \n",
    "    text=tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        tokenize=False, \n",
    "        continue_final_message=True,\n",
    "        #add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    # tokenize inputs\n",
    "    input_ids=tokenizer(text,return_tensors=\"pt\").input_ids\n",
    "    input_ids=input_ids.to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=1024,  # Adjust based on your needs\n",
    "            temperature=0.3,      # Control randomness (lower = more deterministic)\n",
    "            top_p=0.9,            # Take the top logits with probs summing up to 0.9\n",
    "            do_sample=True,\n",
    "            streamer = TextStreamer(tokenizer, skip_special_tokens=True)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful assistant. You first think through the reasoning process step-by-step in your mind and then provide the answer.\n",
      "user\n",
      "Use the following pieces of context to answer the question at the end. If the context is None, simply answer the question based on your knowledge. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
      "    Show your reasoning in <think> </think> tags. And return the final answer in <answer> </answer> tags. \n",
      "    \n",
      "    Question: In today's field day challenge, the 4th graders were competing against the 5th graders. \n",
      "Each grade had 2 different classes. The first 4th grade class had 12 girls and 13 boys. \n",
      "The second 4th grade class had 15 girls and 11 boys. \n",
      "The first 5th grade class had 9 girls and 13 boys while the second 5th grade class had 10 girls and 11 boys. \n",
      "In total, how many more boys were competing than girls?\n",
      "    Helpful Answer:\n",
      "assistant\n",
      "Let me solve this step by step.\n",
      "<think> First, "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll calculate the total number of boys and girls from both grades. Then, I'll find the difference between the total number of boys and girls. </think>\n",
      "The total number of boys is:\n",
      "12 (4th grade class 1) + 13 (4th grade class 2) + 9 (5th grade class 1) + 11 (5th grade class 2) = 45 boys\n",
      "\n",
      "The total number of girls is:\n",
      "15 (4th grade class 2) + 9 (5th grade class 1) + 10 (5th grade class 2) = 34 girls\n",
      "\n",
      "Now, to find out how many more boys were competing than girls, I'll subtract the total number of girls from the total number of boys:\n",
      "45 boys - 34 girls = 11 more boys than girls\n",
      "\n",
      "<answer>11</answer>\n"
     ]
    }
   ],
   "source": [
    "question= \"\"\"In today's field day challenge, the 4th graders were competing against the 5th graders. \n",
    "Each grade had 2 different classes. The first 4th grade class had 12 girls and 13 boys. \n",
    "The second 4th grade class had 15 girls and 11 boys. \n",
    "The first 5th grade class had 9 girls and 13 boys while the second 5th grade class had 10 girls and 11 boys. \n",
    "In total, how many more boys were competing than girls?\"\"\"\n",
    "inference(base_model,tokenizer,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
