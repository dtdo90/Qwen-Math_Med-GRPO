{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e3cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the finetuned model\n",
    "def load_model_tokenizer_ft(base_model_id=\"Qwen/Qwen2.5-3B-Instruct\", adapter_path=\"grpo_lora\"):\n",
    "    # Load the base model\n",
    "    base_model, _ = FastLanguageModel.from_pretrained(\n",
    "        model_name=base_model_id,\n",
    "        max_seq_length=1024,         # Same as during training\n",
    "        load_in_4bit=True,           # Load in 4-bit for memory efficiency\n",
    "    )\n",
    "\n",
    "    # Apply LoRA weights (need to be the same as before)\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        base_model,\n",
    "        r=16,  # Same rank as during training\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "        lora_alpha=32,  # Same alpha as during training\n",
    "        use_gradient_checkpointing=\"unsloth\",  # Enable gradient checkpointing if needed\n",
    "        random_state=3407,  # Same random state as during training\n",
    "    )\n",
    "    # Load the saved LoRA weights\n",
    "    model.load_adapter(adapter_path,adapter_name=\"default\")\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "model,tokenizer=  load_model_tokenizer_ft()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5387e069",
   "metadata": {},
   "source": [
    "## 1. Prompt routing with embedding model\n",
    "When a question is given, it is important to tell the model which knowledge it needs to use. Since the model is equiped with math and medical knowledge, there will be 3 choices\n",
    "- math\n",
    "- medical\n",
    "- others\n",
    "\n",
    "We will use word embedding to determine which type of system_prompt (math, medical, others) should be applied on the given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cbbfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SentenceTransformer as a LangChain-compatible embedding function\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c17239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "prompt_names=[\"math\", \"medical\", \"others\"]\n",
    "\n",
    "prompt_template_math=(\n",
    "    \"You are an expert in solving math problems. \"\n",
    "    \"You first think through the reasoning process step-by-step in your mind and then provide the user with the answer.\"\n",
    ")\n",
    "\n",
    "prompt_template_medical=(\n",
    "    \"You a medical expert with advanced knowledge in clinical reasoning, diagonstics, and treatment planning. \"\n",
    "    \"You first think through the reasoning process step-by-step in your mind and then provide the user with the answer.\"\n",
    ")\n",
    "\n",
    "prompt_template_others = (\n",
    "    f\"You are great at answering all questions not from the following themes: {prompt_names[:-1]}. \"\n",
    "    \"You first think through the reasoning process step-by-step in your mind and then provide the user with the answer.\"\n",
    ")\n",
    "\n",
    "prompt_templates = [\n",
    "    prompt_template_math,\n",
    "    prompt_template_medical,\n",
    "    prompt_template_others,\n",
    "]\n",
    "prompt_embeddings = embedding.embed_documents(prompt_templates)\n",
    "prompt_embeddings=np.asarray(prompt_embeddings)\n",
    "print(\"Embedding dimension: \",len(prompt_embeddings[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb1f4a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_prompt(question):\n",
    "    \"\"\" Select prompt based on question's content: use embedding model to compute the distance between question and prompt templates\n",
    "        Return prompt templates and temperature for generation\n",
    "        Lower temperature for math response (more deterministic)\n",
    "    \"\"\"\n",
    "    question_embedding=embedding.embed_documents([question])\n",
    "    question_embedding=np.asarray(question_embedding)\n",
    "    scores=question_embedding@prompt_embeddings.T\n",
    "    selected_prompt_idx=scores.squeeze(0).argmax()\n",
    "    # math index is 0\n",
    "    if selected_prompt_idx==0: # lower temperature 0.1 for math prompt\n",
    "        return prompt_templates[selected_prompt_idx],0.1\n",
    "    # little higher temperature 0.4 for medical prompt\n",
    "    elif selected_prompt_idx==1:\n",
    "        return prompt_templates[selected_prompt_idx],0.4\n",
    "    # default temperature 0.7 for the rest\n",
    "    else:\n",
    "        return prompt_templates[selected_prompt_idx],0.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ba3f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\"\n",
    "print(\"Selected prompt:\\n\", select_prompt(question)[0])\n",
    "print(\"Temperature: \", select_prompt(question)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a2c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "def generate(question):\n",
    "    system_prompt,temperature=select_prompt(question)\n",
    "    user_prompt=(\n",
    "        f\"Solve the following question:\\n {question}\\n\"\n",
    "        \"Show your reasoning in <think> </think> tags. And return the final answer in <answer> </answer> tags.\\n\"\n",
    "    )\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": \"Let me solve this step by step.\\n<think>\"}]\n",
    "    # chat tempate\n",
    "    inputs=tokenizer.apply_chat_template(messages, tokenize=False, continue_final_message=True)\n",
    "    # tokenize\n",
    "    model_inputs=tokenizer([inputs],return_tensors=\"pt\").to(model.device)\n",
    "    # generate\n",
    "    generated_ids=model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=1024,  # Adjust for longer responses\n",
    "        temperature=temperature,      # lower for more deterministic\n",
    "        streamer = TextStreamer(tokenizer, skip_special_tokens=True)\n",
    "    )\n",
    "    # only record the part generated by the model\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    # decode to text\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9452a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# math question\n",
    "question=\"Baldur gets water from a well. He gets 5 pails of water every morning and 6 pails of water every afternoon. If each pail contains 5 liters of water, how many liters of water does he get every day?\"\n",
    "response=generate(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12582849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# medical question\n",
    "question=\"Which type of thyroid cancer is commonly associated with conditions such as pancreatitis, pituitary tumor, and pheochromocytoma?\"\n",
    "response=generate(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff63cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other types\n",
    "question=\"What is the capital of Vietnam? Answer it short\"\n",
    "response=generate(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f2387e",
   "metadata": {},
   "source": [
    "## 2. Prompt routing with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5de9364",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_prompt_llm(model, tokenizer, question):\n",
    "    # system_message as few-shot prompt\n",
    "    system_message = \"\"\"\\\n",
    "    You are a text classifier.  Given an input text, respond with exactly one word: math, medical, or others.  Output only that wordâ€”no extra text.\n",
    "\n",
    "    Examples:\n",
    "    Text: What is the capital of Vietnam?\n",
    "    Category: others\n",
    "\n",
    "    Text: What are symptoms of Covid?\n",
    "    Category: medical\n",
    "\n",
    "    Text: How to solve a quadratic equation?\n",
    "    Category: math\"\"\"\n",
    "\n",
    "    user_message=(\n",
    "        \"Now classify this:\\n\"\n",
    "        f\"Text: {question}\\nCategory: \"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\",   \"content\": user_message}\n",
    "    ]\n",
    "\n",
    "    # apply chat template and tokenize\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        continue_final_message=True\n",
    "    )\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate at most 2 tokens \n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=2,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        temperature=0.0,\n",
    "        do_sample=False,\n",
    "        streamer=TextStreamer(tokenizer, skip_special_tokens=True)\n",
    "    )\n",
    "    # only record the part generated by the model\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    # decode to text\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfca075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"What is the pythagoras theorem?\"\n",
    "response=select_prompt_llm(model,tokenizer,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea03a534",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_type_temperature={\"math\": 0.1, \"medical\": 0.4, \"others\": 0.7}\n",
    "prompt_type_system={\"math\": prompt_template_math, \"medical\": prompt_template_medical, \"others\": prompt_template_others}\n",
    "\n",
    "\n",
    "def generate_llm_routing(question):\n",
    "    print(\"Determining the type of question ...\")\n",
    "    question_type=select_prompt_llm(model,tokenizer,question)\n",
    "    \n",
    "    # get system prompt and temperature\n",
    "    system_prompt=prompt_type_system[question_type]\n",
    "    temperature=prompt_type_temperature[question_type]\n",
    "    user_prompt=(\n",
    "        f\"Solve the following question:\\n {question}\\n\"\n",
    "        \"Show your reasoning in <think> </think> tags. And return the final answer in <answer> </answer> tags.\\n\"\n",
    "    )\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": \"Let me solve this step by step.\\n<think>\"}]\n",
    "    # chat tempate\n",
    "    inputs=tokenizer.apply_chat_template(messages, tokenize=False, continue_final_message=True)\n",
    "    # tokenize\n",
    "    model_inputs=tokenizer([inputs],return_tensors=\"pt\").to(model.device)\n",
    "    # generate\n",
    "    generated_ids=model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,           # Adjust for longer responses\n",
    "        temperature=temperature,      # lower for more deterministic\n",
    "        streamer = TextStreamer(tokenizer, skip_special_tokens=True)\n",
    "    )\n",
    "    # only record the part generated by the model\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    # decode to text\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e462f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal question\n",
    "question=\"How to treat Covid?\"\n",
    "response=generate_llm_routing(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN_M1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
