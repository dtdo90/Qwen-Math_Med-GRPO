{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Reinforcement Learning on Math Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets trl peft bitsandbytes==0.45.0 \n",
    "!pip install --upgrade pip\n",
    "!pip install unsloth unsloth_zoo --no-cache-dir --upgrade\n",
    "!pip install vllm\n",
    "!pip install --upgrade pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def extract_hash_answer(text):\n",
    "  # the final answer comes after ####\n",
    "  if \"####\" not in text:\n",
    "    return None\n",
    "\n",
    "  return text.split(\"####\")[1].strip()\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an expert in solving high school math word problems. \"\n",
    "    \"You first think through the reasoning process step-by-step in your mind and then provide the answer.\"\n",
    ")\n",
    "\n",
    "user_prompt= (\n",
    "  \"Solve the following question:\\n{question}.\\n\"\n",
    "  \"Show your work in <think> </think> tags. And return the final answer as an integer in <answer> </answer> tags.\"\n",
    ")\n",
    "\n",
    "def format_prompt(tokenizer,sample):\n",
    "  prompt=[\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "      {\"role\": \"user\", \"content\": user_prompt.format(question=sample[\"question\"])},\n",
    "      {\"role\": \"assistant\", \"content\": \"Let me solve this step by step.\\n<think>\"}\n",
    "  ]\n",
    "  return {\n",
    "      \"prompt\": tokenizer.apply_chat_template(prompt,tokenize=False, continue_final_message=True),\n",
    "      \"target\": extract_hash_answer(sample[\"answer\"]),\n",
    "  }\n",
    "\n",
    "def get_questions(tokenizer):\n",
    "  \"\"\" To train with GRPO, each data sample needs to have fields \"prompt\" and \"answer\"\n",
    "      Given dataset: \"question\" and \"answer\"\n",
    "  \"\"\"\n",
    "  ds = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "  ds = ds.map(lambda x: format_prompt(tokenizer,x))\n",
    "  # Optional: remove redundant columns\n",
    "  # ds = ds.remove_columns(['question', 'answer'])\n",
    "  return ds\n",
    "\n",
    "# extract answer from the model to verify correctness\n",
    "def extract_xml_answer(text):\n",
    "  if \"<answer>\" not in text or \"</answer>\" not in text:\n",
    "    return None\n",
    "  answer=text.split(\"<answer>\")[1].strip()    # strip white spaces (\\ or \\n) both front and back\n",
    "  answer=answer.split(\"</answer>\")[0].strip()\n",
    "  return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n",
    "\n",
    "dataset=get_questions(tokenizer)\n",
    "train_dataset, test_dataset=dataset[\"train\"], dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Reward functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def format_reward(completions, target, **kwargs):\n",
    "    \"\"\"\n",
    "    Format: <think>...</think><answer>...</answer>\n",
    "    Args:\n",
    "        completions (list[str]): Generated outputs\n",
    "        target (list[str]): Expected answers\n",
    "\n",
    "    Returns:\n",
    "        list[float]: Reward scores\n",
    "    \"\"\"\n",
    "    rewards=[]\n",
    "    for completion,gt_ans in zip(completions,target):\n",
    "      try: # add synthetic <think> as it is part of the prompt\n",
    "        completion = \"<think>\"+completion\n",
    "        # define regular expression to check if the format is correct\n",
    "        # allow nested tags like <think><other_tag>...</other_tag></think> but not nested <think> or </think> \n",
    "        regex = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\n<answer>([\\s\\S]*?)<\\/answer>$\"\n",
    "\n",
    "        match=re.search(regex,completion,re.DOTALL)\n",
    "        if match is None or len(match.groups())!=2:\n",
    "          rewards.append(0.0)\n",
    "        else: \n",
    "          # (Optional) print to monitor model's response\n",
    "          # print('-'*100, f\"\\nModel Response:\\n{completion}\", f\"\\n#### Answer: {gt_ans}\")\n",
    "          rewards.append(1.0)\n",
    "      except Exception:\n",
    "        rewards.append(0.0)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def correctness_reward(completions,target,**kwargs) -> list[float]:\n",
    "  \"\"\"\n",
    "    Args:\n",
    "        completions (list[str]): Generated outputs\n",
    "        target (list[str]): Expected answers, each is an integer, but read a string\n",
    "\n",
    "    Returns:\n",
    "        list[float]: Reward scores\n",
    "  \"\"\"\n",
    "  rewards=[]\n",
    "  for completion, gt_ans in zip(completions,target):\n",
    "    model_ans=extract_xml_answer(completion)\n",
    "    # Check if model_ans is None (i.e., no <answer> tag found)\n",
    "    if model_ans is None:\n",
    "        rewards.append(0.0)  # No answer found, so reward is 0\n",
    "        continue\n",
    "    try:\n",
    "        # Compare the model's answer to the ground truth\n",
    "        if abs(float(model_ans) - float(gt_ans)) < 1e-5:\n",
    "            rewards.append(1.0)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "    except Exception: # Handle cases where model_ans or gt_ans cannot be converted to float\n",
    "        rewards.append(0.0)\n",
    "  return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Load model with unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1678/1722887075.py:1: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel, is_bfloat16_supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 03-13 04:14:10 __init__.py:207] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.3.9: Fast Qwen2 patching. Transformers: 4.49.0. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.168 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit with actual GPU utilization = 59.39%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 22.17 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 1024. Num Sequences = 224.\n",
      "Unsloth: vLLM's KV Cache can use up to 10.75 GB. Also swap space = 6 GB.\n",
      "INFO 03-13 04:14:17 config.py:549] This model supports multiple tasks: {'reward', 'generate', 'classify', 'embed', 'score'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.2.mlp', 'model.layers.3.mlp', 'model.layers.30.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 03-13 04:14:17 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":224}, use_cached_outputs=False, \n",
      "INFO 03-13 04:14:17 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 03-13 04:14:18 model_runner.py:1110] Starting to load model unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit...\n",
      "INFO 03-13 04:14:18 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W313 04:14:18.705141233 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-13 04:14:18 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5708846797482ea02a7d1e2eb852c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5194f7c5806549ac95ccd4baf51a93d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-13 04:14:20 model_runner.py:1115] Loading model weights took 2.2160 GB\n",
      "INFO 03-13 04:14:20 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 03-13 04:14:22 worker.py:267] Memory profiling takes 2.25 seconds\n",
      "INFO 03-13 04:14:22 worker.py:267] the current vLLM instance can use total_gpu_memory (22.17GiB) x gpu_memory_utilization (0.59) = 13.17GiB\n",
      "INFO 03-13 04:14:22 worker.py:267] model weights take 2.22GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 1.23GiB; the rest of the memory reserved for KV Cache is 9.68GiB.\n",
      "INFO 03-13 04:14:22 executor_base.py:111] # cuda blocks: 17617, # CPU blocks: 10922\n",
      "INFO 03-13 04:14:22 executor_base.py:116] Maximum concurrency for 1024 tokens per request: 275.27x\n",
      "INFO 03-13 04:14:28 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:26<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-13 04:14:55 model_runner.py:1562] Graph capturing finished in 27 secs, took 4.41 GiB\n",
      "INFO 03-13 04:14:55 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 35.04 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth 2025.3.9 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "import torch\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    max_seq_length = 1024, # Can increase for longer reasoning traces\n",
    "    load_in_4bit = True,   # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    gpu_memory_utilization = 0.6, # Reduce if out of memory\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,      # Larger rank = smarter, but slower. Suggested values 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ], # Remove QKVO if out of memory\n",
    "    lora_alpha = 32,\n",
    "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 7,473 | Num Epochs = 1 | Total steps = 400\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 29,933,568/1,830,055,936 (1.64% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 4:12:20, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completion_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / correctness_reward</th>\n",
       "      <th>rewards / format_reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>212.500000</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>0.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.441942</td>\n",
       "      <td>184.625000</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>202.125000</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.156250</td>\n",
       "      <td>0.839689</td>\n",
       "      <td>202.187500</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.062500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>196.562500</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.437500</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>175.125000</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.441942</td>\n",
       "      <td>159.250000</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>183.875000</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>181.062500</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>218.562500</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>209.281250</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>214.718750</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>178.593750</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>0.397748</td>\n",
       "      <td>190.625000</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>187.531250</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.031250</td>\n",
       "      <td>0.574524</td>\n",
       "      <td>196.812500</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.437500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>189.625000</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.486136</td>\n",
       "      <td>222.750000</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>169.406250</td>\n",
       "      <td>0.001743</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>0.309359</td>\n",
       "      <td>180.312500</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>0.397748</td>\n",
       "      <td>182.687500</td>\n",
       "      <td>0.002072</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>199.093750</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>192.343750</td>\n",
       "      <td>0.001939</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>0.397748</td>\n",
       "      <td>172.531250</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>0.486136</td>\n",
       "      <td>186.937500</td>\n",
       "      <td>0.002541</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>0.574524</td>\n",
       "      <td>204.906250</td>\n",
       "      <td>0.002536</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>0.486136</td>\n",
       "      <td>176.656250</td>\n",
       "      <td>0.005030</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.281250</td>\n",
       "      <td>0.574524</td>\n",
       "      <td>200.343750</td>\n",
       "      <td>0.002613</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>197.593750</td>\n",
       "      <td>0.003285</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.468750</td>\n",
       "      <td>0.220971</td>\n",
       "      <td>166.656250</td>\n",
       "      <td>0.005296</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.343750</td>\n",
       "      <td>0.397748</td>\n",
       "      <td>192.593750</td>\n",
       "      <td>0.003737</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>174.468750</td>\n",
       "      <td>0.005642</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>205.562500</td>\n",
       "      <td>0.004599</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>169.968750</td>\n",
       "      <td>0.004240</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.718750</td>\n",
       "      <td>0.309359</td>\n",
       "      <td>167.250000</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.281250</td>\n",
       "      <td>0.662913</td>\n",
       "      <td>197.062500</td>\n",
       "      <td>0.003681</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>0.220971</td>\n",
       "      <td>193.843750</td>\n",
       "      <td>0.005761</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>164.937500</td>\n",
       "      <td>0.004743</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.468750</td>\n",
       "      <td>0.397748</td>\n",
       "      <td>195.937500</td>\n",
       "      <td>0.004676</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>0.486136</td>\n",
       "      <td>189.968750</td>\n",
       "      <td>0.004374</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.441942</td>\n",
       "      <td>168.125000</td>\n",
       "      <td>0.003787</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.781250</td>\n",
       "      <td>0.220971</td>\n",
       "      <td>172.156250</td>\n",
       "      <td>0.003938</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>0.397748</td>\n",
       "      <td>188.875000</td>\n",
       "      <td>0.002496</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>0.574524</td>\n",
       "      <td>184.750000</td>\n",
       "      <td>0.003181</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>0.397748</td>\n",
       "      <td>185.187500</td>\n",
       "      <td>0.002931</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>169.812500</td>\n",
       "      <td>0.003018</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.656250</td>\n",
       "      <td>0.397748</td>\n",
       "      <td>174.093750</td>\n",
       "      <td>0.004021</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>0.132583</td>\n",
       "      <td>188.906250</td>\n",
       "      <td>0.002534</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.593750</td>\n",
       "      <td>0.220971</td>\n",
       "      <td>198.437500</td>\n",
       "      <td>0.002677</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>198.281250</td>\n",
       "      <td>0.002816</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>167.593750</td>\n",
       "      <td>0.003408</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.437500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>173.218750</td>\n",
       "      <td>0.003022</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>0.309359</td>\n",
       "      <td>184.937500</td>\n",
       "      <td>0.003767</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.468750</td>\n",
       "      <td>0.220971</td>\n",
       "      <td>180.687500</td>\n",
       "      <td>0.005720</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>182.031250</td>\n",
       "      <td>0.003773</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>190.750000</td>\n",
       "      <td>0.003083</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>170.562500</td>\n",
       "      <td>0.002945</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>0.088388</td>\n",
       "      <td>191.281250</td>\n",
       "      <td>0.002487</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>217.250000</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>0.468750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.437500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>193.937500</td>\n",
       "      <td>0.003556</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.437500</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>193.125000</td>\n",
       "      <td>0.003988</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.656250</td>\n",
       "      <td>0.397748</td>\n",
       "      <td>175.718750</td>\n",
       "      <td>0.003183</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>0.220971</td>\n",
       "      <td>188.312500</td>\n",
       "      <td>0.003640</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.593750</td>\n",
       "      <td>0.309359</td>\n",
       "      <td>186.937500</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>204.250000</td>\n",
       "      <td>0.002916</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>185.250000</td>\n",
       "      <td>0.004319</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>174.062500</td>\n",
       "      <td>0.003492</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.156250</td>\n",
       "      <td>0.486136</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>0.002572</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>0.486136</td>\n",
       "      <td>184.531250</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>186.500000</td>\n",
       "      <td>0.002513</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>189.093750</td>\n",
       "      <td>0.002931</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>0.088388</td>\n",
       "      <td>188.750000</td>\n",
       "      <td>0.001959</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>0.441942</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>0.002925</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>166.250000</td>\n",
       "      <td>0.003523</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.281250</td>\n",
       "      <td>0.397748</td>\n",
       "      <td>202.812500</td>\n",
       "      <td>0.003066</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.437500</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>171.250000</td>\n",
       "      <td>0.003663</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.437500</td>\n",
       "      <td>0.441942</td>\n",
       "      <td>181.437500</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>0.486136</td>\n",
       "      <td>206.656250</td>\n",
       "      <td>0.003243</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>170.281250</td>\n",
       "      <td>0.002546</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>0.309359</td>\n",
       "      <td>198.500000</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.437500</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>195.656250</td>\n",
       "      <td>0.003407</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>0.397748</td>\n",
       "      <td>183.125000</td>\n",
       "      <td>0.003734</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.437500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>195.250000</td>\n",
       "      <td>0.004591</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.593750</td>\n",
       "      <td>0.220971</td>\n",
       "      <td>176.656250</td>\n",
       "      <td>0.002428</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.062500</td>\n",
       "      <td>0.441942</td>\n",
       "      <td>187.812500</td>\n",
       "      <td>0.003734</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>348</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>180.656250</td>\n",
       "      <td>0.005281</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>196.031250</td>\n",
       "      <td>0.002704</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>356</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.468750</td>\n",
       "      <td>0.309359</td>\n",
       "      <td>182.812500</td>\n",
       "      <td>0.002803</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>0.309359</td>\n",
       "      <td>188.187500</td>\n",
       "      <td>0.003231</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>364</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.343750</td>\n",
       "      <td>0.486136</td>\n",
       "      <td>193.656250</td>\n",
       "      <td>0.002888</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>201.093750</td>\n",
       "      <td>0.002567</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.468750</td>\n",
       "      <td>0.486136</td>\n",
       "      <td>176.593750</td>\n",
       "      <td>0.004711</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>202.312500</td>\n",
       "      <td>0.002376</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.468750</td>\n",
       "      <td>0.397748</td>\n",
       "      <td>177.375000</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.656250</td>\n",
       "      <td>0.309359</td>\n",
       "      <td>174.625000</td>\n",
       "      <td>0.003163</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>388</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.593750</td>\n",
       "      <td>0.309359</td>\n",
       "      <td>187.812500</td>\n",
       "      <td>0.002437</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>166.500000</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>396</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>180.437500</td>\n",
       "      <td>0.004075</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>0.309359</td>\n",
       "      <td>190.562500</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: cc054318-ba17-4a32-99b1-6087497a45dd)') - silently ignoring the lookup for the file config.json in unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = GRPOConfig(\n",
    "    use_vllm = True, # use vLLM for fast inference!\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    logging_steps = 4,                  # log information every 4 steps\n",
    "    bf16 = is_bfloat16_supported(),     # determine the data type used for training\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 4,  # Increase to 4 for smoother training\n",
    "    num_generations = 2,              # Decrease if out of memory\n",
    "    max_prompt_length = 256,\n",
    "    max_completion_length = 256,\n",
    "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_steps = 400,\n",
    "    save_steps = 400,\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir = \"grpo_lora\",\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        correctness_reward,\n",
    "        format_reward,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# save model and tokenizer\n",
    "trainer.model.save_lora(training_args.output_dir)\n",
    "trainer.tokenizer.save_pretrained(training_args.output_dir)\n",
    "\n",
    "# delete model, trainer and free cache\n",
    "del model, trainer\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import unsloth\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "\n",
    "def load_model_tokenizer_ft(base_model_id, adapter_path):\n",
    "  \"\"\" base_model_id = id of the based model on HF\n",
    "      adapter_path = saved lora weights\n",
    "  \"\"\"\n",
    "  # Load the base model\n",
    "  model, _ = FastLanguageModel.from_pretrained(\n",
    "      model_name=base_model_id,\n",
    "      max_seq_length=1024,         # Same as during training\n",
    "      load_in_4bit=True,           # Load in 4-bit for memory efficiency\n",
    "      fast_inference=True,         # Enable fast inference\n",
    "      gpu_memory_utilization=0.8,  # Adjust based on your GPU memory\n",
    "  )\n",
    "\n",
    "  # Apply LoRA weights (need to be the same as before)\n",
    "  model = FastLanguageModel.get_peft_model(\n",
    "      model,\n",
    "      r=16,  # Same rank as during training\n",
    "      target_modules=[\n",
    "          \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "          \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "      ],\n",
    "      lora_alpha=32,  # Same alpha as during training\n",
    "      use_gradient_checkpointing=\"unsloth\",  # Enable gradient checkpointing if needed\n",
    "      random_state=3407,  # Same random state as during training\n",
    "  )\n",
    "  # Load the saved LoRA weights\n",
    "  model.load_adapter(adapter_path,adapter_name=\"default\")\n",
    "  # Set the model to evaluation mode\n",
    "  model.eval()\n",
    "\n",
    "  # Load the tokenizer\n",
    "  tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "  return model, tokenizer\n",
    "\n",
    "\n",
    "# load model and tokenizer\n",
    "# base_model_id = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "# adapter_path = \"grpo_lora\"  # Directory where LoRA weights and tokenizer are saved\n",
    "# model,tokenizer=load_model_tokenizer_ft(base_model_id, adapter_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(data, i):\n",
    "  \"\"\" Inference on the i-th sample of the test data\"\"\"\n",
    "\n",
    "  # load model and tokenizer\n",
    "\n",
    "  # prompt from test data\n",
    "  prompt=data[i][\"prompt\"]\n",
    "  target=data[i][\"target\"]\n",
    "\n",
    "  # tokenize the prompt\n",
    "  input_ids=tokenizer(prompt,return_tensors=\"pt\").input_ids\n",
    "  input_ids=input_ids.to(model.device)\n",
    "  input_length=input_ids.shape[1]\n",
    "\n",
    "  with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=512,  # Adjust based on your needs\n",
    "        temperature=0.1,  # Controls randomness (lower = more deterministic)\n",
    "        top_p=0.9,        # Take the top logits with probs summing up to 0.9\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "  output_ids_generate = output_ids[:, input_length:]\n",
    "  outputs_text=tokenizer.decode(output_ids_generate[0], skip_special_tokens=True)\n",
    "  full_text=prompt+outputs_text\n",
    "  print(full_text)\n",
    "  print(\"Target: \",target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an expert in solving high school math word problems. You first think through the reasoning process step-by-step in your mind and then provide the answer.<|im_end|>\n",
      "<|im_start|>user\n",
      "Solve the following question:\n",
      "Baldur gets water from a well. He gets 5 pails of water every morning and 6 pails of water every afternoon. If each pail contains 5 liters of water, how many liters of water does he get every day?.\n",
      "Show your work in <think> </think> tags. And return the final answer as an integer in <answer> </answer> tags.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Let me solve this step by step.\n",
      "<think> Baldur gets 5 pails of water in the morning and 6 pails in the afternoon. Each pail contains 5 liters of water. To find out how many liters of water he gets every day, I need to first calculate the total number of pails he gets in a day, and then multiply that by the number of liters each pail contains. </think>\n",
      "To find the total number of pails Baldur gets in a day, I add the pails he gets in the morning to the pails he gets in the afternoon:\n",
      "5 pails (morning) + 6 pails (afternoon) = 11 pails per day\n",
      "Now, I multiply the total number of pails by the number of liters each pail contains:\n",
      "11 pails * 5 liters/pail = 55 liters\n",
      "So, Baldur gets 55 liters of water every day.\n",
      "<answer>55</answer>\n",
      "Target:  55\n"
     ]
    }
   ],
   "source": [
    "inference(test_dataset, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Further instruction finetuning\n",
    "\n",
    "We will further finetune the model train with GRPO on a dataset by deepseek."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 03-13 10:23:51 __init__.py:207] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.3.9: Fast Qwen2 patching. Transformers: 4.49.0. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA L40S. Num GPUs = 1. Max memory: 44.527 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit with actual GPU utilization = 39.59%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 44.53 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 1024. Num Sequences = 256.\n",
      "Unsloth: vLLM's KV Cache can use up to 15.21 GB. Also swap space = 6 GB.\n",
      "INFO 03-13 10:23:59 config.py:549] This model supports multiple tasks: {'classify', 'embed', 'score', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.2.mlp', 'model.layers.3.mlp', 'model.layers.30.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 03-13 10:23:59 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-13 10:23:59 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 03-13 10:24:00 model_runner.py:1110] Starting to load model unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit...\n",
      "INFO 03-13 10:24:00 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W313 10:24:00.048043033 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-13 10:24:00 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56eb3824b77a447ebb818be4e7f9b742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4393e8949d4b45c888a040253ee47682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-13 10:24:01 model_runner.py:1115] Loading model weights took 2.2160 GB\n",
      "INFO 03-13 10:24:02 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 03-13 10:24:04 worker.py:267] Memory profiling takes 2.15 seconds\n",
      "INFO 03-13 10:24:04 worker.py:267] the current vLLM instance can use total_gpu_memory (44.53GiB) x gpu_memory_utilization (0.40) = 17.63GiB\n",
      "INFO 03-13 10:24:04 worker.py:267] model weights take 2.22GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 1.41GiB; the rest of the memory reserved for KV Cache is 13.92GiB.\n",
      "INFO 03-13 10:24:04 executor_base.py:111] # cuda blocks: 25346, # CPU blocks: 10922\n",
      "INFO 03-13 10:24:04 executor_base.py:116] Maximum concurrency for 1024 tokens per request: 396.03x\n",
      "INFO 03-13 10:24:10 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:32<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-13 10:24:43 model_runner.py:1562] Graph capturing finished in 33 secs, took 5.08 GiB\n",
      "INFO 03-13 10:24:43 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 41.22 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth 2025.3.9 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import unsloth\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "\n",
    "def load_model_tokenizer_ft(base_model_id, adapter_path):\n",
    "  \"\"\" base_model_id = id of the based model on HF\n",
    "      adapter_path = saved lora weights\n",
    "  \"\"\"\n",
    "  # Load the base model\n",
    "  model, _ = FastLanguageModel.from_pretrained(\n",
    "      model_name=base_model_id,\n",
    "      max_seq_length=1024,         # Same as during training\n",
    "      load_in_4bit=True,           # Load in 4-bit for memory efficiency\n",
    "      fast_inference=True,         # Enable fast inference\n",
    "      gpu_memory_utilization=0.4,  # Adjust based on your GPU memory\n",
    "  )\n",
    "\n",
    "  # Apply LoRA weights (need to be the same as before)\n",
    "  model = FastLanguageModel.get_peft_model(\n",
    "      model,\n",
    "      r=16,  # Same rank as during training\n",
    "      target_modules=[\n",
    "          \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "          \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "      ],\n",
    "      lora_alpha=32,  # Same alpha as during training\n",
    "      use_gradient_checkpointing=\"unsloth\",  # Enable gradient checkpointing if needed\n",
    "      random_state=3407,  # Same random state as during training\n",
    "  )\n",
    "  # Load the saved LoRA weights\n",
    "  model.load_adapter(adapter_path,adapter_name=\"default\")\n",
    "  # Set the model to evaluation mode\n",
    "  model.eval()\n",
    "\n",
    "  # Load the tokenizer\n",
    "  tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "  return model, tokenizer\n",
    "  \n",
    "# load model and tokenizer\n",
    "base_model_id = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "adapter_path = \"grpo_lora\"  # Directory where LoRA weights and tokenizer are saved\n",
    "model,tokenizer=load_model_tokenizer_ft(base_model_id, adapter_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You a medical expert with advanced knowledge in clinical reasoning, diagonstics, and treatment planning.\"\n",
    "    \"You first think through the reasoning process step-by-step in your mind and then provide the user with the answer.\"\n",
    ")\n",
    "\n",
    "user_prompt= (\n",
    "    \"Below is a question that describes the condition of a patient. Write a response that appropriately answers the question.\"\n",
    "    \"Show your reasoning in <think> </think> tags. And return the final response in <answer> </answer> tags.\\n\"\n",
    "    \"###Question###:\\n{question}\\n\"\n",
    "    \"###Response###:\\n<think>\"\n",
    ")\n",
    "\n",
    "def format_sample(sample):\n",
    "    question = sample[\"Question\"]\n",
    "    cot = sample[\"Complex_CoT\"]\n",
    "    response = sample[\"Response\"]\n",
    "\n",
    "    # Format the prompt using the tokenizer's chat template\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt.format(question=question)},\n",
    "            {\"role\": \"assistant\", \"content\": f\"{cot}\\n</think>\\n<answer>\\n{response}\\n</answer>\\n\"},\n",
    "        ],\n",
    "        tokenize=False,  # Do not tokenize yet\n",
    "        continue_final_message=True\n",
    "    )\n",
    "\n",
    "    return {\"prompt\": formatted_prompt} \n",
    "\n",
    "\n",
    "def gen_dataset(num_samples=10000):\n",
    "    \"\"\" Return:\n",
    "            A generator on train data \"train_gen\"\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\", split = \"train\",trust_remote_code=True)\n",
    "\n",
    "    # take 2000 data, shuffle with a fixed seed for reproducibility\n",
    "    dataset = dataset.shuffle(seed=42).select(range(num_samples))\n",
    "    dataset=dataset.map(format_sample)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset=gen_dataset()\n",
    "# split train and test data\n",
    "dataset=dataset.train_test_split(test_size=0.1)\n",
    "ds_train, ds_test=dataset[\"train\"], dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys:  dict_keys(['Question', 'Complex_CoT', 'Response', 'prompt'])\n",
      "{'Question': 'What drug is used specifically for the reversal of cerebral vasospasm and infarct following a subarachnoid hemorrhage?', 'Complex_CoT': \"Okay, so let's think about subarachnoid hemorrhage first. It's when there's bleeding around the brain, and I know that it brings a set of complications along with it. One major issue is cerebral vasospasm. This sounds pretty serious because it can actually lead to something worse, like a stroke or permanent brain damage. Keeping that in mind, it makes sense that we need to find a way to stop this vasospasm from happening. \\n\\nMoving on, what kind of treatment do we have? I remember reading that maintaining blood pressure and hydration is important, but those are more like general support measures. What about medications? They should have something specific for vasospasm. Hmm, there must be drugs that target the blood vessels directly to prevent them from constricting. \\n\\nOh, right, there's this drug called Nimodipine. I've heard it's a calcium channel blocker which specifically works on the brainâ€™s blood vessels. That's kind of neat because it means it's not just a general blood pressure pill, but something that targets the problem area directly. \\n\\nAnd it seems Nimodipine isnâ€™t just for preventing vasospasm. It's supposed to help improve outcomes by reducing the chances of brain infarction after SAH. So, like, it keeps the blood flow steady and prevents those bad constrictions that could lead to an infarct. \\n\\nI'm pretty sure this is backed up by a lot of research and guidelines. It would make sense since doctors would need solid evidence to use it reliably in patients. \\n\\nSo yeah, it definitely looks like Nimodipine is the go-to medication for dealing with vasospasm following a subarachnoid hemorrhage. It's interesting how a specific drug can make such a significant difference.\", 'Response': 'The drug specifically used for the management of cerebral vasospasm following a subarachnoid hemorrhage is Nimodipine. Nimodipine is a calcium channel blocker that helps prevent the narrowing of blood vessels in the brain, thereby reducing the risk of a secondary stroke and improving outcomes by decreasing the likelihood of cerebral infarction.', 'prompt': \"<|im_start|>system\\nYou a medical expert with advanced knowledge in clinical reasoning, diagonstics, and treatment planning.You first think through the reasoning process step-by-step in your mind and then provide the user with the answer.<|im_end|>\\n<|im_start|>user\\nBelow is a question that describes the condition of a patient. Write a response that appropriately answers the question.Show your reasoning in <think> </think> tags. And return the final response in <answer> </answer> tags.\\n###Question###:\\nWhat drug is used specifically for the reversal of cerebral vasospasm and infarct following a subarachnoid hemorrhage?\\n###Response###:\\n<think><|im_end|>\\n<|im_start|>assistant\\nOkay, so let's think about subarachnoid hemorrhage first. It's when there's bleeding around the brain, and I know that it brings a set of complications along with it. One major issue is cerebral vasospasm. This sounds pretty serious because it can actually lead to something worse, like a stroke or permanent brain damage. Keeping that in mind, it makes sense that we need to find a way to stop this vasospasm from happening. \\n\\nMoving on, what kind of treatment do we have? I remember reading that maintaining blood pressure and hydration is important, but those are more like general support measures. What about medications? They should have something specific for vasospasm. Hmm, there must be drugs that target the blood vessels directly to prevent them from constricting. \\n\\nOh, right, there's this drug called Nimodipine. I've heard it's a calcium channel blocker which specifically works on the brainâ€™s blood vessels. That's kind of neat because it means it's not just a general blood pressure pill, but something that targets the problem area directly. \\n\\nAnd it seems Nimodipine isnâ€™t just for preventing vasospasm. It's supposed to help improve outcomes by reducing the chances of brain infarction after SAH. So, like, it keeps the blood flow steady and prevents those bad constrictions that could lead to an infarct. \\n\\nI'm pretty sure this is backed up by a lot of research and guidelines. It would make sense since doctors would need solid evidence to use it reliably in patients. \\n\\nSo yeah, it definitely looks like Nimodipine is the go-to medication for dealing with vasospasm following a subarachnoid hemorrhage. It's interesting how a specific drug can make such a significant difference.\\n</think>\\n<answer>\\nThe drug specifically used for the management of cerebral vasospasm following a subarachnoid hemorrhage is Nimodipine. Nimodipine is a calcium channel blocker that helps prevent the narrowing of blood vessels in the brain, thereby reducing the risk of a secondary stroke and improving outcomes by decreasing the likelihood of cerebral infarction.\\n</answer>\\n\"}\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "sample=ds_train[0]\n",
    "print(\"Keys: \",sample.keys())\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load trained model, tokenizer and fine tune further\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29,933,568 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 9,000 | Num Epochs = 2 | Total steps = 562\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 29,933,568/1,830,055,936 (1.64% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='562' max='562' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [562/562 1:05:46, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.830300</td>\n",
       "      <td>1.377714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.319500</td>\n",
       "      <td>1.299790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.286000</td>\n",
       "      <td>1.282762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.282400</td>\n",
       "      <td>1.272671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.266900</td>\n",
       "      <td>1.265567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.256400</td>\n",
       "      <td>1.260582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.253400</td>\n",
       "      <td>1.257197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.247100</td>\n",
       "      <td>1.254936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.248800</td>\n",
       "      <td>1.253613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.246200</td>\n",
       "      <td>1.253012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.242300</td>\n",
       "      <td>1.252947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen2ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('qwen-grpo-deepseek/tokenizer_config.json',\n",
       " 'qwen-grpo-deepseek/special_tokens_map.json',\n",
       " 'qwen-grpo-deepseek/vocab.json',\n",
       " 'qwen-grpo-deepseek/merges.txt',\n",
       " 'qwen-grpo-deepseek/added_tokens.json',\n",
       " 'qwen-grpo-deepseek/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and tokenizer (aleady loaded!)\n",
    "# base_model_id = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "# adapter_path = \"grpo_lora\"  # Directory where LoRA weights and tokenizer are saved\n",
    "# model,tokenizer=load_model_tokenizer_ft(base_model_id, adapter_path)\n",
    "\n",
    "import os\n",
    "\n",
    "# put model to train mode\n",
    "model.train()\n",
    "print(f\"{sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters\")\n",
    "\n",
    "training_args=SFTConfig(\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4, # gradient accumulation\n",
    "    per_device_eval_batch_size=16, # should set to 8 for faster speed!!!\n",
    "    logging_steps=50,              # log train loss every 50 steps\n",
    "    evaluation_strategy=\"steps\",   # Evaluate at the end of each epoch\n",
    "    eval_steps=50,                 # Evaluate every 50 steps    \n",
    "    num_train_epochs = 2,          # or set max_steps=200\n",
    "    warmup_ratio=0.03,             # follow QLoRA paper\n",
    "    learning_rate=3e-5,\n",
    "    bf16=True,        # train in bf16 for higher precision\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    output_dir=\"qwen-grpo-deepseek\",\n",
    "    dataset_text_field=\"prompt\",\n",
    "    report_to=\"none\",\n",
    "    seed=42, # for reproducibility\n",
    ")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class =tokenizer,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_test,\n",
    "    args=training_args,\n",
    ")\n",
    "os.environ['UNSLOTH_RETURN_LOGITS'] = '1'\n",
    "trainer.train()\n",
    "\n",
    "# save model and tokenizer\n",
    "trainer.model.save_lora(training_args.output_dir)\n",
    "trainer.tokenizer.save_pretrained(training_args.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# delete model, trainer and free cache\n",
    "del model, trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.9: Fast Qwen2 patching. Transformers: 4.49.0. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA L40S. Num GPUs = 1. Max memory: 44.527 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit with actual GPU utilization = 63.13%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 44.53 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 1024. Num Sequences = 320.\n",
      "Unsloth: vLLM's KV Cache can use up to 25.69 GB. Also swap space = 6 GB.\n",
      "INFO 03-13 09:36:38 config.py:549] This model supports multiple tasks: {'classify', 'score', 'embed', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.2.mlp', 'model.layers.3.mlp', 'model.layers.30.mlp'], 'llm_int8_threshold': 6.0}\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.2.mlp', 'model.layers.3.mlp', 'model.layers.30.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 03-13 09:36:38 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":320}, use_cached_outputs=False, \n",
      "INFO 03-13 09:36:39 model_runner.py:1110] Starting to load model unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit...\n",
      "INFO 03-13 09:36:39 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "INFO 03-13 09:36:39 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9adb0fe634f4498ac55a1ec36290083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b99291f9ac4630877c97da5a1503b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-13 09:36:40 model_runner.py:1115] Loading model weights took 2.2082 GB\n",
      "INFO 03-13 09:36:41 worker.py:267] Memory profiling takes 0.59 seconds\n",
      "INFO 03-13 09:36:41 worker.py:267] the current vLLM instance can use total_gpu_memory (44.53GiB) x gpu_memory_utilization (0.63) = 28.11GiB\n",
      "INFO 03-13 09:36:41 worker.py:267] model weights take 2.21GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.74GiB; the rest of the memory reserved for KV Cache is 24.17GiB.\n",
      "INFO 03-13 09:36:41 executor_base.py:111] # cuda blocks: 43991, # CPU blocks: 10922\n",
      "INFO 03-13 09:36:41 executor_base.py:116] Maximum concurrency for 1024 tokens per request: 687.36x\n",
      "INFO 03-13 09:36:42 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:41<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-13 09:37:23 model_runner.py:1562] Graph capturing finished in 42 secs, took 0.12 GiB\n",
      "INFO 03-13 09:37:23 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 43.27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_model_tokenizer_ft(base_model_id, adapter_path):\n",
    "  \"\"\" base_model_id = id of the based model on HF\n",
    "      adapter_path = saved lora weights\n",
    "  \"\"\"\n",
    "  # Load the base model\n",
    "  model, _ = FastLanguageModel.from_pretrained(\n",
    "      model_name=base_model_id,\n",
    "      max_seq_length=1024,         # Same as during training\n",
    "      load_in_4bit=True,           # Load in 4-bit for memory efficiency\n",
    "      fast_inference=True,         # Enable fast inference\n",
    "      gpu_memory_utilization=0.6,  # Adjust based on your GPU memory\n",
    "  )\n",
    "\n",
    "  # Apply LoRA weights (need to be the same as before)\n",
    "  model = FastLanguageModel.get_peft_model(\n",
    "      model,\n",
    "      r=16,  # Same rank as during training\n",
    "      target_modules=[\n",
    "          \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "          \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "      ],\n",
    "      lora_alpha=32,  # Same alpha as during training\n",
    "      use_gradient_checkpointing=\"unsloth\",  # Enable gradient checkpointing if needed\n",
    "      random_state=3407,  # Same random state as during training\n",
    "  )\n",
    "  # Load the saved LoRA weights\n",
    "  model.load_adapter(adapter_path,adapter_name=\"default\")\n",
    "  # Set the model to evaluation mode\n",
    "  model.eval()\n",
    "\n",
    "  # Load the tokenizer\n",
    "  tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "  return model, tokenizer\n",
    "\n",
    "# Load fine-tuned model and tokenizer \n",
    "base_model_id = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "adapter_path = \"qwen-grpo-deepseek\"  # Directory where LoRA weights and tokenizer are saved\n",
    "model,tokenizer=load_model_tokenizer_ft(base_model_id, adapter_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>system\\nYou a medical expert with advanced knowledge in clinical reasoning, diagonstics, and treatment planning.You first think through the reasoning process step-by-step in your mind and then provide the user with the answer.<|im_end|>\\n<|im_start|>user\\nBelow is a question that describes the condition of a patient. Write a response that appropriately answers the question.Show your reasoning in <think> </think> tags. And return the final response in <answer> </answer> tags.\\n###Question###:\\nA 40-year old man presented with acute onset pain and swelling of left great toe. On X-ray, punched out lytic lesion seen on phalanx with sclerotic margins and overhanging bony edges. Diagnosis is:(AIIMS November 2014, November 2013)\\nA. Gout\\nB. Rheumatoid arthritis\\nC. Psoriatic arthritis\\nD. Reiter's syndrome\\n###Response###:\\n<think><|im_end|>\\n<|im_start|>assistant\\nAlright, let's see what's going on here.\\n\\nSo, we've got a 40-year-old guy who suddenly starts feeling pain and notices swelling in his left big toe? That's pretty classic for something like gout. That's the kind of thing you'd expect when uric acid goes rogue and crystals start forming in the joint.\\n\\nNow, what do we have on the X-ray? A 'punched out' lesion on the bone? And it's got these sclerotic margins and what's described as 'overhanging bony edges.' Those terms remind me of something. Oh right, the famous 'rat bite' sign, which is typical for gout!\\n\\nLet's think about other possibilities just to be sure. Rheumatoid arthritis? Hmm, I remember that's more about symmetrical joint pain and involves stuff like morning stiffness. And it doesn't usually cause these specific bone lesions, especially not right in the big toe.\\n\\nPsoriatic arthritis? While it can involve toe joints too, it doesnâ€™t typically present as an acute and isolated attack on the big toe, with the specific X-ray changes like these. \\n\\nReiter's syndrome? Hmm, that often goes with symptoms like conjunctivitis or urethritis and the arthritis is usually more widespread and not specific to the toe with these kinds of bone changes.\\n\\nSo, based on the toe pain and swelling along with these X-ray findings, it's really pointing to gout. Yes, that makes the most sense. The detail with the bony edges seals the deal.\\n\\nOkay, to wrap it all up, everything aligns perfectly with a diagnosis of gout, specifically podagra. Yeah, it's definitely gout.\\n</think>\\n<answer>\\nThe diagnosis for a 40-year-old man experiencing acute pain and swelling in the left great toe, along with X-ray findings of a punched-out lytic lesion with sclerotic margins and overhanging bony edges, is most consistent with gout (choice A). The described X-ray findings, particularly the 'punched out' lesion with 'overhanging bony edges,' are classic indicators of gout, also known as the 'rat bite' sign. These findings, combined with the clinical presentation involving the big toe, strongly point to gout, specifically podagra.\\n</answer>\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ds_test[0]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      " A 60-year-old man with a 1-year history of recurrent aspiration pneumonia, unconscious and gasping for air, dies despite resuscitative efforts. His autopsy shows degeneration of the corticospinal tracts, anterior horn cells of the upper cervical cord, and asymmetrical atrophy of limb muscles, diaphragm, and intercostal muscles. Based on this clinical presentation and autopsy findings, which drug would have most likely slowed the progression of his neurodegenerative disease?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model response:\n",
      " <think>\n",
      "Okay, so we have this 60-year-old guy who's been dealing with a lot of trouble with his lungs lately. He's been having these episodes of aspiration pneumonia, and he's been unconscious and gasping for air. Despite trying to help him, he died. Hmm, that's really sad. So, let's think about what's going on here. He has this degeneration in the corticospinal tracts, which are those pathways that connect the brain to the spinal cord. This suggests there's a problem with motor control.\n",
      "\n",
      "Also, there's atrophy in the muscles like the diaphragm and intercostal muscles. These are muscles involved in breathing, so that's pretty significant. This atrophy suggests there might be a problem with motor neurons, like the anterior horn cells in the upper cervical cord. I remember these are often linked to diseases like ALS, where motor neurons in the spinal cord are affected.\n",
      "\n",
      "And then, there's the degeneration in the corticospinal tracts. That's usually seen in conditions where there's a problem with the upper motor neurons. I'm thinking, maybe he's got something like amyotrophic lateral sclerosis, which is also known as ALS. ALS is a neurodegenerative disease that affects these upper motor neurons, leading to muscle weakness and eventually paralysis.\n",
      "\n",
      "Now, thinking about what could slow the progression of this disease, it's about protecting those motor neurons. I know that certain drugs, like riluzole, have been studied for ALS. Riluzole works by inhibiting the release of glutamate, which is a neurotransmitter that can be harmful to motor neurons. By reducing glutamate, it can potentially slow down the progression of ALS.\n",
      "\n",
      "So, it seems like riluzole might be the drug that would slow down the progression of his neurodegenerative disease. Yeah, that makes sense. The drug that could have slowed the progression of his ALS is riluzole.\n",
      "</think>\n",
      "<answer>\n",
      "Based on the clinical presentation and autopsy findings, it is likely that the 60-year-old man was suffering from amyotrophic lateral sclerosis (ALS), a neurodegenerative disease characterized by the degeneration of upper motor neurons. ALS is known to progress by damaging the motor neurons, leading to muscle weakness and eventually paralysis. The drug that would most likely slow the progression of his neurodegenerative disease is riluzole. Riluzole works by inhibiting the release of glutamate, which is a neurotransmitter that can be harmful to motor neurons. By reducing glutamate, riluzole can potentially slow the progression of ALS.\n",
      "</answer>\n",
      "</think>\n",
      "</think>\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target response:\n",
      " <think>\n",
      "Alright, let's think about this. We have a case where a man has had recurrent aspiration pneumonia. Thatâ€™s interesting because it can often point towards problems with swallowing, which might mean there's something going on with his nervous system, particularly the bulbar region.\n",
      "\n",
      "Now, what's really getting my attention is that autopsy report. It mentions degeneration of the corticospinal tracts and the anterior horn cells. These findings are pretty telltale, suggesting that both upper and lower motor neurons are affected. Hmm, isnâ€™t there a condition where both of these are commonly involved? Oh yes! Amyotrophic Lateral Sclerosis, commonly known as ALS or sometimes Lou Gehrig's disease.\n",
      "\n",
      "ALS is characterized by these very symptoms of muscle atrophy and both upper and lower motor neuron degeneration. The muscle atrophy, especially in limb muscles and crucial ones like the diaphragm, indicates progressive involvement.\n",
      "\n",
      "Now, the big question: what can we do to help a patient like this? I remember from what I've read that riluzole is a drug that's often used in ALS. Why riluzole, you ask? Well, it's known to work by reducing glutamate-induced excitotoxicity, which basically means it helps protect neurons from getting overexcited and damaged.\n",
      "\n",
      "Riluzole doesnâ€™t cure ALS, but it does slow the progression, which could be crucial for a patient to preserve some function or extend survival. In this unfortunate case, if he had received riluzole earlier, it might have provided a slight extension of life or a delay in disease progression, which makes it really important.\n",
      "\n",
      "So, putting it all together, with the presentation of symptoms and the findings from the autopsy pointing towards ALS, riluzole is definitely the drug that wouldâ€™ve been used to try and slow things down a bit. This conclusion lines up with the guidelines and treatment practices for ALS.\n",
      "</think>\n",
      "<answer>\n",
      "Based on the clinical presentation and autopsy findings, the drug that would have most likely slowed the progression of his neurodegenerative disease, which is indicated to be Amyotrophic Lateral Sclerosis (ALS), is riluzole. Riluzole works by reducing glutamate-induced excitotoxicity and is commonly used to slow the progression of ALS, potentially extending survival and preserving muscle function for a longer period.\n",
      "</answer>\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# inference function used earlier\n",
    "def inference(data, i):\n",
    "  \"\"\" Inference on the i-th sample of the test data\"\"\"\n",
    "\n",
    "  # prompt from test data: separate the answer\n",
    "  sample=data[i]['prompt']\n",
    "  question=sample.split(\"###Question###:\")[1].strip()\n",
    "  question=question.split(\"###Response###\")[0].strip()\n",
    "\n",
    "  # correct response (for comparison)\n",
    "  correct_response=\"<think>\"+sample.split(\"<|im_start|>assistant\")[1]\n",
    "\n",
    "  # add back <|im_start|>assistant for the model to generate\n",
    "  prompt=sample.split(\"<|im_start|>assistant\")[0]+\"<|im_start|>assistant\"\n",
    "\n",
    "  # tokenize the prompt: set add_special_tokens=False because we already did it\n",
    "  input_ids=tokenizer(prompt,return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "  input_ids=input_ids.to(model.device)\n",
    "  input_length=input_ids.shape[1]\n",
    "\n",
    "  with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=1024,  # Adjust based on your needs\n",
    "        temperature=0.7,  # Controls randomness (lower = more deterministic)\n",
    "        top_p=0.9,        # Take the top logits with probs summing up to 0.9\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "  output_ids_generate = output_ids[:, input_length:]\n",
    "  outputs_text=tokenizer.decode(output_ids_generate[0], skip_special_tokens=True)\n",
    "\n",
    "  # prepend <think> token\n",
    "  model_response=\"<think>\"+outputs_text\n",
    "\n",
    "  print(\"Question:\\n\", question)\n",
    "  print(\"-\"*100)\n",
    "  print(\"Model response:\\n\", model_response)\n",
    "  print(\"-\"*100)\n",
    "  print(\"Target response:\\n\", correct_response)\n",
    "  print(\"-\"*100)\n",
    "\n",
    "\n",
    "\n",
    "inference(ds_test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
